# Visual Place Recognition & Uncertainty Estimation Project

This project evaluates various Visual Place Recognition (VPR) methods and implements a post-hoc **Uncertainty Estimation** module (Section 6.2). It uses geometric verification (Inliers) to predict the confidence of VPR retrievals, enabling robust localization even in challenging cross-domain scenarios (e.g., Day vs. Night).

---

## üõ† 1. Environment Setup (Installation)

### 1.1 Python Environment
Ensure you have **Python 3.10** installed.

```bash
# Create virtual environment
py -3.10 -m venv venv

# Activate it
.\venv\Scripts\activate  # Windows
# source venv/bin/activate # Linux/Mac

# Install dependencies
pip install -r requirements.txt
```

### 1.2 Git Submodules & SuperGlue Fix
This project relies on `image-matching-models` and `imatch-toolbox`. Follow these steps to fix submodule paths for proper installation:

```bash
# 1. Update the main submodule
git submodule update --init --recursive

# 2. Apply SuperGlue Fix
cd image-matching-models
git submodule update --init matching/third_party/imatch-toolbox

# 3. Apply SuperGlue Fix
cd matching\third_party\imatch-toolbox
```

#### Action: Open .gitmodules in the imatch-toolbox directory and replace its content with the following to ensure valid URLs:
```bash


[submodule "third_party/SuperGluePretrainedNetwork"]
	path = third_party/superglue
	url = [https://github.com/magicleap/SuperGluePretrainedNetwork.git](https://github.com/magicleap/SuperGluePretrainedNetwork.git)
[submodule "third_party/r2d2"]
	path = third_party/r2d2
	url = [https://github.com/naver/r2d2.git](https://github.com/naver/r2d2.git)
	ignore = untracked
[submodule "third_party/d2net"]
	path = third_party/d2net
	url = [https://github.com/mihaidusmanu/d2-net.git](https://github.com/mihaidusmanu/d2-net.git)
[submodule "third_party/patch2pix"]
	path = third_party/patch2pix
	url = [https://github.com/GrumpyZhou/patch2pix.git](https://github.com/GrumpyZhou/patch2pix.git)
```

Finalize Setup:
```bash
# Run inside imatch-toolbox folder
git submodule update --init
```


---


## üîç 2. Uncertainty Estimation (Section 6.2 Methodology)

This module implements a probabilistic uncertainty estimation mechanism. Instead of relying solely on similarity scores, we use **Geometric Verification** as a proxy for confidence.

### 2.1 Methodology
We treat uncertainty estimation as a binary classification problem solvable via **Logistic Regression**:
1.  **Retrieval:** The VPR model (e.g., MegaLoc, CosPlace) retrieves top candidates.
2.  **Matching:** We apply **SuperPoint + LightGlue** (Sparse) or **LoFTR** (Dense) to match the query with the retrieved image.
3.  **Feature:** The number of geometrically verified inliers is used as the input feature ($x$).
4.  **Prediction:** A Logistic Regression model predicts the probability of correctness ($P(y=1|x)$).

### 2.2 Experiment Design: Cross-Domain Robustness
To ensure robustness, we designed a challenging Cross-Domain experiment:
* **Training (The "Teacher"):** **SVOX (Sun vs. Night)**. Trained on difficult lighting conditions to force the model to learn a conservative decision boundary.
* **Testing (The "Student"):** **SF-XS (San Francisco)** / **Tokyo**. Evaluated on urban datasets to validate generalization.


  ---

## üöÄ 3. Execution Guide (Step-by-Step Instructions)

Follow this pipeline to reproduce the results for Section 6.2.

### Phase 1: Data Generation (Retrieval & Matching)

**Step 1.1: Run VPR Retrieval**
Generate predictions and save data for uncertainty analysis (`z_data.torch`).
*(Example using MegaLoc on SF-XS)*

```bash
python VPR-methods-evaluation/main.py \
--num_workers 8 \
--batch_size 32 \
--log_dir log_dir \
--method=cosplace --backbone=ResNet18 --descriptors_dimension=512 \
--image_size 512 512 \
--database_folder 'D:\AML\Visual-Place-Recognition-Project\data\tokyo_xs\test\database' \
--queries_folder 'D:\AML\Visual-Place-Recognition-Project\data\tokyo_xs\test\queries' \
--num_preds_to_save 20 \
--recall_values 1 5 10 20 \
--save_for_uncertainty
```

**Step 1.2: Run Feature Matching**
Run matching to calculate inliers. To satisfy course requirements, we compare 4 matchers, for example:

* **SuperPoint + LightGlue (Sparse)**
    ```bash
    python match_queries_preds.py ^
        --preds-dir 'D:/Visual-Place-Recognition-Project/logs/log_dir/<TIMESTAMP>/preds' \
		--matcher superpoint-lg \
		--num-preds 20
    ```

### Phase 2: Qualitative Analysis (Plots)

**Goal:** Generate S-Curves (Fig 1), Calibration Curves (Fig 2), and Comparison Charts (Fig 3).

1.  Open `universal_lr.py`.
2.  Update `TRAIN_LOG_DIR` (SVOX path) and `TEST_LOG_DIRS` (New experiment path).
3.  Set `MATCHER_FOLDER = "preds_superpoint-lg"` (or `"preds_loftr"`).
4.  Run:
    ```bash
    python universal_lr.py
    ```
5.  **Output:** Figures will be saved in the ../Visual-Place-Recognition-Project/results.

### Phase 3: Quantitative Analysis (Benchmarking)

**Goal:** Compare our **Geometric Uncertainty (Inliers)** against baselines (L2, PA-Score, SUE).

```bash
python -m vpr_uncertainty.eval \
	--preds-dir '<path-to-predictions-folder>' \
	--inliers-dir '<path-to-inliers-folder>' \
	--z-data-path '<path-to-z-data-file>'
```
**Expected Output Like:**
```text
L2-distance: 99.1
PA-score:    96.7
SUE (GPS):   97.6
Inliers:     99.0  <-- Our Proposed Method
```
### üí° Result Interpretation (Key Finding)

 **`Inliers: 99.0 <-- Our Proposed Method`**

This line represents the core achievement of our **Uncertainty Estimation** module (Section 6.2).

* **Method:** We use the number of **Geometric Inliers** (matches verified by SuperPoint+LightGlue) as the proxy for confidence.
* **Performance (99.0%):** The **AUPRC** score of 99.0% indicates that "Number of Inliers" is a highly reliable predictor. It outperforms traditional baselines like **Spatial Uncertainty Estimation (SUE, 97.6%)** and **Feature Distance (PA-Score, 96.7%)**.
* **Conclusion:** This validates that geometric verification is the most robust way to estimate uncertainty in Visual Place Recognition, effectively distinguishing between correct and incorrect localizations.


---

## üìä 4. Comprehensive Results

### 4.1 Tokyo Dataset Benchmark
Full comparison of all VPR methods with different matchers on the **Tokyo** dataset:

|Dataset | Method | Backbone | Dim | Matcher | R@1 | R@5 | R@10 | R@20 | AUPRC | AURCpa | AURCsue | AURCrand | AURCinl |
|:---| :--- | :--- | :--- | :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| **tokyo** | **CosPlace** | ResNet18 | 512 | superpoint-lg | 82.9 | 86.7 | 88.3 | 89.5 | 92.4 | 90.2 | 82.5 | 66.1 | 99.0 |
| | | | | loftr | 84.8 | 87.9 | 88.6 | 89.5 | 92.4 | 90.2 | 82.5 | 66.1 | 99.2 |
| | | | | superglue | 82.9 | 87.0 | 88.3 | 89.5 | 92.4 | 90.2 | 82.5 | 66.1 | 99.0 |
| | **Megaloc** | Dinov2 | 8448 | superpoint-lg | 94.3 | 98.4 | 98.7 | 99.0 | 99.5 | 99.3 | 97.8 | 94.9 | 99.8 |
| | | | | loftr | 94.6 | 97.8 | 98.7 | 99.0 | 99.5 | 99.3 | 97.8 | 94.9 | 99.8 |
| | | | | superglue | 93.7 | 98.4 | 98.7 | 99.0 | 99.5 | 99.3 | 97.8 | 94.9 | 99.6 |
| | **NetVLAD** | VGG16 | 4096 | superpoint-lg | 68.6 | 71.7 | 72.7 | 74.6 | 74.1 | 76.8 | 77.0 | 53.1 | 98.5 |
| | | | | loftr | 68.9 | 71.7 | 72.7 | 74.6 | 74.1 | 76.8 | 77.0 | 53.1 | 98.7 |
| | | | | superglue | 67.0 | 70.8 | 73.0 | 74.6 | 74.1 | 76.8 | 77.0 | 53.1 | 98.3 |
| | **MixVPR** | ResNet50 | 512 | superpoint-lg | 87.9 | 91.4 | 92.1 | 93.3 | 97.3 | 94.9 | 88.4 | 75.2 | 99.4 |
| | | | | loftr | 89.8 | 91.7 | 92.7 | 93.3 | 97.3 | 94.9 | 88.4 | 75.2 | 99.6 |
| | | | | superglue | 87.0 | 91.1 | 92.4 | 93.3 | 97.3 | 94.9 | 88.4 | 75.2 | 99.4 |

### 4.2 Uncertainty Estimation Analysis (SF-XS)
Results from `universal_lr.py` evaluating the uncertainty module on **SF-XS** (Trained on SVOX (Sun vs Night)):

| Method       | AUPRC (Robustness) ‚Üë | AUSE (Calibration) ‚Üì | Spearman (Ranking) ‚Üë | R¬≤ Score (Fit) ‚Üë |
| :---         | :---                 | :---                 | :---                 | :---             |
| **CosPlace** | 0.8611               | 0.0954               | 0.5820               | 0.0242           |
| **NetVLAD** | 0.7339               | 0.1319               | **0.6015** | **0.2217** |
| **MixVPR** | 0.8980               | 0.0755               | 0.5674               | -0.1384          |
| **MegaLoc** | **0.9742** | **0.0231** | 0.4652               | -1.4816* |

Results from `universal_lr.py` evaluating the uncertainty module on **TOKYO-XS** (Trained on SVOX (Sun vs Night)):

| Method       | AUPRC (Robustness) ‚Üë | AUSE (Calibration) ‚Üì | Spearman (Ranking) ‚Üë | R¬≤ Score (Fit) ‚Üë |
| :---         | :---                 | :---                 | :---                 | :---             |
| **CosPlace** | 0.8583               | 0.1017               | 0.5158               | -0.1749          |
| **NetVLAD**  | 0.9213               | 0.0453               | **0.7334**               | **0.1148**           |
| **MixVPR**   | 0.9303               | 0.0560               | 0.4818               | -0.6831          |
| **MegaLoc**  | 0.**9950**               | **0.0047**               | 0.2900               | -7.4204*         |

*\*Note: The negative R¬≤ score for MegaLoc is due to the domain shift between training (Night) and testing (Day). While the model is conservative (under-estimating probabilities), it maintains excellent ranking performance (High AUPRC), effectively distinguishing correct matches from incorrect ones.*

---

## üìÇ Repository Structure

* `VPR-methods-evaluation/`: Main framework for running VPR retrieval.
* `universal_lr.py`: Core script for Logistic Regression training & plotting.
* `vpr_uncertainty/`: Package containing baseline evaluation logic (`eval.py`).
* `match_queries_preds.py`: Script for feature matching (SuperPoint, LoFTR).
* `logs/`: Directory storing experiment results.
